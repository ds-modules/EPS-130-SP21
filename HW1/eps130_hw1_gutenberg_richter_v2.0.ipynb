{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Instructions\n",
    "The assignments in this course will all be interactive jupyter notebooks with python code. The notebook is a series of cells which you can edit and run. Markdown cells will contain background info, instructions, and prompts to answer questions. Code cells will contain pre-written support code and prompts to fill in your own code solutions. You should run all of the code cells in order, even the ones you don't edit, since we'll often refer back to functions and variables defined in previous cells. Some questions will ask you to change the parameters of a previous calculation--rather than duplicating the whole code cell, you should make these changes in the existing code and comment out the original/other parameters like this:\n",
    "\n",
    "```python\n",
    "# s = np.arange(0, 10, 1) # original sampling\n",
    "s = np.arange(0, 10, 0.1) # sampling at 10x higher frequency\n",
    "\n",
    "```\n",
    "\n",
    "We're using a package called otter to help with grading. In a few places you'll be able to check your code results with otter tests. At the end of the notebook, otter will help you output a pdf of your assignment to submit to bCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Occurrence Statistics\n",
    "\n",
    "The statistics of earthquake occurrence is revealed from catalogs of seismicity, which include event time, location and magnitude. We will talk about how earthquakes are located and how magnitudes are estimated separately, but for now it is sufficient to know that this information can be easily acquired. With such catalogs it is possible to compare the seismic activity of different regions, make informed assessments about the frequency of earthquake occurrence, and learn about the fault rupture process. Maps of the earthquakes in catalogs over time reveal the structure of faulting in a region, and provide a framework with which to study the seismotectonics of a region.\n",
    "\n",
    "There are two primary earthquake statistics used by seismologists. They are the Gutenberg-Richter relationship (Gutenberg and Richter, 1949), and the Omori Law (Omori, 1894). \n",
    "\n",
    "Gutenberg and Richter found that when the logarithm of the number of earthquakes is plotted vs. magnitude that the distribution (data) may be described by the line (model), log(N)=A+Bm, where N is the number of earthquakes, m is the magnitude and A (y-intercept) and B (slope) are refered to as the Gutenberg-Richter statistics or coefficients. They found that on a global scale, and subsequently more generally, the B-value or the slope of the Gutenberg-Richter line is approximately equal to -1. Thus for each increase in earthquake magnitude there are approximately 10 times fewer earthquakes. If, for example, there are 100 M3 events in a region per year, then the Gutenberg-Richter relationship generally finds that there would be approximately 10 M4 events and 1 M5 event in each year. For magnitudes larger than M5 there would be fewer than one event per year. Gutenberg-Richter is a very important earthquake statistic because it is used to determine the rates of earthquake occurrence, which is a key step in characterizing earthquake hazards (we will see this in future homework exercises). \n",
    "\n",
    "The Omori Law is used to characterize the rate at which aftershocks occur following a large mainshock event. This statistic is used for comparing the aftershock productivity of different earthquakes and regions, make forecasts of the likelihood of large damaging aftershocks and to distinguish between earthquake faulting and possibly geothermal or volcanic-related seismicity by examining whether the distribution describes a \"mainshock/aftershock\" pattern or is more \"swarm-like\". \n",
    "\n",
    "In this homework you will use python code in this notebook to investigate the Gutenberg-Richter and Omori statistics for the San Francisco Bay Area, as well as develop numerical analysis skills using python. \n",
    "\n",
    "Note: This is not a python class, but the primary programming tool that will be used is python. However, if you know MatLab or have other programing background and would prefer to use it, you are free to use those tools instead. It will be helpful to read sections 9.6 and 9.8 of Lay and Wallace (1995) prior to working on this laboratory for background on the Gutenberg-Richter relation and the Omori Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Initial Setup and Subroutine Definitions\n",
    "# !pip install cartopy -q\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.shapereader import Reader as cReader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two geographic points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371.0 * c\n",
    "    return km\n",
    "\n",
    "def countDays(c,y,m,d):\n",
    "    '''\n",
    "    Function to count days in the array\n",
    "    '''\n",
    "    days=np.zeros(c)\n",
    "    for i in range(0,c,1):\n",
    "        d0 = datetime.date(y[0], m[0], d[0])\n",
    "        d1 = datetime.date(y[i], m[i], d[i])\n",
    "        delta = d1 - d0\n",
    "        days[i]=delta.days\n",
    "    return days\n",
    "\n",
    "def readAnssCatalog(p):\n",
    "    '''\n",
    "    Function to slice an ANSS catalog loaded as a pandas dataframe and return arrays of info, including days\n",
    "    '''\n",
    "    d=np.array(p)         # load the dataframe into numpy as an array      \n",
    "    year=d[:,0].astype(int)  # define variables from the array\n",
    "    month=d[:,1].astype(int)\n",
    "    day=d[:,2].astype(int)\n",
    "    hour=d[:,3].astype(int)\n",
    "    minute=d[:,4].astype(int)\n",
    "    sec=d[:,5].astype(int)\n",
    "    lat=d[:,6]\n",
    "    lon=d[:,7]\n",
    "    mag=d[:,8]\n",
    "    days = countDays(len(year),year,month,day)\n",
    "    return year,month,day,hour,minute,sec,lat,lon,mag,days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Catalog\n",
    "We have downloaded the Advanced National Seismic System (ANSS) catalog from 1900 to 2018 for you to use (also available here: http://www.quake.geo.berkeley.edu/anss/catalog-search.html), and saved it as a text-file named \"anss_catalog_1900to2018all.txt\". This catalog has all events in the aforementioned time range located within 100 km of UC Berkeley. Columns of this catalog include information about the catalogued earthquakes, including the date and time of each event, its location in latitude, longitude and depth, and the event magnitude.  \n",
    "\n",
    "The following python code reads this catalog file and places the information in arrays for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data and create data arrays\n",
    "\n",
    "# This catalog is a M0+ search centered at Berkeley radius=100. \n",
    "# big enough to include Loma Prieta but exclude Geysers\n",
    "data=pd.read_csv('anss_catalog_1900to2018all.txt', sep=' ', delimiter=None, header=None,\n",
    "                names = ['Year','Month','Day','Hour','Min','Sec','Lat','Lon','Mag'])\n",
    "\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Explore the raw catalog (10 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Print basic catalog info\n",
    "Find the number of events, the number of days from the first event, the minimum magnitude, and the maximum magnitude. Use the variable names provided so that the example print statements and autograding will work.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nevt = len(year)\n",
    "ndays = ...\n",
    "min_mag = ...\n",
    "max_mag = ...\n",
    "print(f'There are {nevt} events in the catalog.')\n",
    "print(f'The first event was {ndays:.0f} days before the last.')\n",
    "print(f'The magnitudes range from {min_mag} to {max_mag}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot the catalog time series\n",
    "Make an x-y scatter plot showing the magnitude of the earthquake on the y-axis and the time of the event on the x-axis. For this it is convenient that the last output of the `readAnssCatalog` function is the number of days since the beginning of the catalog. The plot will show that the catalog is not uniform due to the fact that over time as more seismic recording stations were installed more earthquakes could be detected and properly located.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.2\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(..., ...,'.')\n",
    "ax.set(xlabel='days', ylabel='magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "# fig.savefig(\"hw1_ex1_fig1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot the catalog in map view\n",
    "Familiarize yourself with the code example below as there will be additional exercises requiring the plotting of maps. The code is not quite complete. Fill in the missing parts of the `plt.scatter()` and `plt.plot()` calls.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.3\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "ydim=10      #height of plot\n",
    "xdim=ydim*(haversine_np(lon0,lat0,lon1,lat0)/haversine_np(lon0,lat0,lon0,lat1)) #scale width\n",
    "\n",
    "###\n",
    "plt.figure(figsize=(ydim,xdim))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='longitude', ylabel='Latitude',\n",
    "       title='Raw Catalog')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES,alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "ax.add_feature(cfeature.STATES.with_scale('10m'))\n",
    "\n",
    "# Plot events as open circles with size and color proportional to event magnitude\n",
    "indx=np.argsort(mag)   #determine sort index #Sort Descending to plot largest events on top\n",
    "x=lon[indx]            #apply sort index\n",
    "y=lat[indx]\n",
    "z=np.exp(mag[indx])    #exponent to scale size\n",
    "c = plt.cm.plasma(z/max(z))\n",
    "plt.scatter(..., ..., s=..., facecolors='none', edgecolors=c, marker='o', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add Berkeley, CA as a red square with size proportional to event magnitude (wiki coords or similar)\n",
    "plt.plot(..., ...,'rs',markersize=10)\n",
    "\n",
    "# Add fault traces --- WAIT to uncomment this section until instructed below\n",
    "# prfx = '../Qfaults_GIS/SHP/'\n",
    "# faults_files = ['Qfaults_US_Database.shp','ca_offshore.shp']\n",
    "# for ff in faults_files:\n",
    "#     shape_feature = cfeature.ShapelyFeature(cReader(prfx+ff).geometries(),\n",
    "#                                     ccrs.PlateCarree(), edgecolor='black')\n",
    "#     ax.add_feature(shape_feature, edgecolor='green', facecolor='')\n",
    "\n",
    "# You can save the plot by calling plt.savefig() BEFORE plt.show()\n",
    "# plt.savefig('hw1_ex2_seismap_raw.pdf')\n",
    "# plt.savefig('hw1_ex2_seismap_raw.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**:\n",
    "\n",
    "- Describe the seismicity and any patterns that you see.\n",
    "- Try adding fault traces by uncommenting the provided code. How well does the seismicity show the region's major faults?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Compute the Gutenberg-Richter statitistics (30 pts)\n",
    "\n",
    "Follow the steps below to compute the Gutenberg Richter statistics for the raw catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Determine and plot the Gutenberg-Richter Distribution\n",
    "First, define a range of magnitudes to bin the data. You can use a range of magnitude, m from 0.0 to 6.9 in increments of 0.1 magnitude unit.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.1\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of mag bins with width 0.1 magnitude per bin\n",
    "m = np.arange(..., ..., ...) # remember that arange uses a \"half-open\" interval [start,stop)\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, count the number of events above a given magnitude. That is count the number of events above and equal to magnitude 0.0, then above and equal to 0.1, and so forth all the way to the maximum magnitude. You can do this by placing the code for vectorized counting of array elements passing a logical test (numpy.count_nonzero()) inside a for loop over the incremental magnitudes, m. We are interested in the annual rate of the events so you will need to divide by the total number of years the catalog spans (hint: this isn't an integer if you account for the entire span of the catalog). N is the log of of the number of events per year, so take the log base 10 (numpy.log10) of the annual number of earthquakes for each magnitude bin. Note you can place all of the operations in one line of code inside the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preallocate the vector logN with a zeros vector of size len(m)\n",
    "logN = ...\n",
    "ny = ...\n",
    "\n",
    "# Find N\n",
    "for i in range(0,len(m),1):\n",
    "    ...\n",
    "    \n",
    "\n",
    "# print(logN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(..., ...,'o')\n",
    "plt.xlabel('mag')\n",
    "plt.ylabel('log(N)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data to find the Gutenberg Richter statistics.\n",
    "\n",
    "Now, fit the data with the Gutenberg Richter relationship $log_{10}(N(m))$=A+Bm. In other words, \"invert\" the data to find the applied model parameters. We will walk through the steps of this inversion.\n",
    "\n",
    "1) First, create the model parameter matrix, G, which has one column of 1's and a second column of magnitude bins, m.\n",
    "\n",
    "$$\n",
    "G=\\begin{pmatrix}\n",
    "1 &m_0\\\\\n",
    "1 &m_1\\\\\n",
    "1 &m_2\\\\\n",
    ". &.\\\\\n",
    ". &.\\\\\n",
    ". &.\\\\\n",
    "1 &m_n\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "2) Next, create the data matrix, d, which in this case is a single column and contains the $log_{10}(N(m))$ values. Note that we already defined this vector above.\n",
    "\n",
    "$$\n",
    "d=\\begin{pmatrix}\n",
    "log_{10}(N(m_0))\\\\\n",
    "log_{10}(N(m_1))\\\\\n",
    "log_{10}(N(m_2))\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "log_{10}(N(m_n))\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.ones(len(logN))        # column of 1's\n",
    "G=np.column_stack((tmp,m))    # matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Next, compute the $G^{T}G$ matrix (G-transpose times G)  using numpy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTG=np.dot(np.transpose(G),G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Next, compute the $G^{T}D$ (A-transpose times D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = logN # Recall that we have already defined the D-matrix above as \"logN\"\n",
    "GTD=np.dot(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5) Finally, solve the inverse problem. Invert the equation $(G^{T}G)x=G^{T}D$ using the numpy linear algebra solver (numpy.linalg.solv()). The result, x, will be a vector of the Gutenberg-Richter coefficients, in which the A-value is x[0] and the B-value is x[1].\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the linear inverse problem\n",
    "soln=np.linalg.solve(..., ...)\n",
    "print(f'A-value={soln[0]:.3f}, B-value={soln[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear Gutenberg-Richter model is fully defined by the A and B coefficients, however in order to plot a line through the distribution we need to take the dot product of G with our two-parameter solution vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=m # the independent variable of the best-fit line is the same as for the data (magnitude bin)\n",
    "y=np.dot(G,soln) # synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector, \"y\", is called the synthetic data because it is a synthetic estimate of the real data. The difference between synthetic data and real data can be quantified through uncertainty analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Uncertainty analysis of Gutenberg-Richter model\n",
    "\n",
    "Next, compute the uncertainties of the model (best-fit line defined by the A and B coefficients). \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.3\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps outline how to compute 95% confidence intervals for the model using the numpy and scipy packages in Python.\n",
    "\n",
    "1) df=(length_of_data) - (number_of_model_parameters) #degree of freedom\n",
    "\n",
    "2) e=data-(model predictions) #prediction error\n",
    "\n",
    "3) variance=np.sum(e*e)/df\n",
    "\n",
    "4) se_y=np.sqrt(var)                       #standard error of the estimate\n",
    "\n",
    "5) sdev=np.sqrt(var)                       #standard deviation\n",
    "\n",
    "6) t=stats.t.ppf(1-0.05/2,degfree)             #two-sided students t-distribution\n",
    "\n",
    "7) lower95=np.exp(np.log(modeled_pga)-t*se_y) \n",
    "\n",
    "8) upper95=np.exp(np.log(modeled_pga)+t*se_y) \n",
    "\n",
    "9) se_b=sdev/np.sqrt(np.sum((x-np.mean(x))\\**2)) # standard error of slope\n",
    "\n",
    "10) se_a=sdev\\*np.sqrt(1/len(x) + np.mean(x)\\*\\*2/np.sum((x-np.mean(x))**2)) # standard error of intercept (9 and 10 will be important for incorporating Gutenberg Richter uncertainty in PSHA (a future homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the uncertainty in Gutenberg-Richter Parameters\n",
    "\n",
    "length_of_data = len(logN)\n",
    "number_of_model_parameters = 2\n",
    "df=(length_of_data) - (number_of_model_parameters) #degree of freedom\n",
    "\n",
    "e=logN-y #prediction error\n",
    "\n",
    "var=np.sum(e*e)/df\n",
    "\n",
    "se_y=np.sqrt(var)             #standard error of the estimate\n",
    "sdev=np.sqrt(var)             #standard deviation\n",
    "\n",
    "#Calculate 95% confidence bounds\n",
    "t=stats.t.ppf(1-0.05/2,df)    #two-sided students t-distribution\n",
    "tmp=np.sqrt(1/len(x)+((x-np.mean(x))**2)/np.sum((x-np.mean(x))**2))\n",
    "tmp=tmp/max(tmp)\n",
    "lower95=y-t*se_y*tmp\n",
    "upper95=y+t*se_y*tmp\n",
    "se_b=sdev/np.sqrt(np.sum((x-np.mean(x))**2))                      #standard error slope\n",
    "se_a=sdev*np.sqrt(1/len(x) + np.mean(x)**2/np.sum((x-np.mean(x))**2)) #standard error of intercept\n",
    "a95=se_a*t\n",
    "b95=se_b*t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fit to the data\n",
    "To visualize the fit to the distribution, we would like to make a plot showing the magnitude distribution as plotted above, log10(N) vs m, with the best fit line that we just found the coefficients for. Visalize the uncertainty by plotting the upper and lower 95 percentiles of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#plot the distribution like above\n",
    "ax.plot(m,logN,'bo')\n",
    "\n",
    "#plot the best-fit line with uncertainties\n",
    "ax.plot(x,y,'r-',linewidth=2)\n",
    "\n",
    "# plot the 95% confidence intervals\n",
    "ax.plot(, ,'k-',linewidth=2)\n",
    "ax.plot(, ,'k-',linewidth=2)\n",
    "\n",
    "ax.set(xlabel='magnitude', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Initial Gutenberg-Richter Distribution')\n",
    "ax.grid()\n",
    "# plt.savefig(\"hw1_ex3_figure2_withErrorBounds.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. What do the Gutenberg Richter statistics represent for the earthquake distribution?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-1\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. What happens to the shape of the distribution (log(N)) if you reduce the magnitude bin size by a factor of 10?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-2\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Why does the model parameter matrix (G) have a column of 1's?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-3\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. What determines the size of $G^TG$ and $G^Td$?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-4\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. How well does the Gutenberg-Richter model fit the data? Quantify your answer in terms of uncertainty.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-5\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "6. Where does the fit begin to breakdown and why?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-6\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "7. Based on your Gutenberg-Richter coefficients what is the annual rates of a M4 earthquakes? For a M7 earthquake?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-7\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "8. On average how many years are there between M7 earthquakes based on this catalog.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-8\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "9. How many M7 earthquakes are in the catalog?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-9\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "10. What is your assessment of the quality or suitability of the forecast of average M7 occurrence?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4-10\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Exercise 3: Declustering (30 pts)\n",
    "\n",
    "In the above analysis mainshocks (primary events) and aftershocks are mixed together. The results for the Gutenberg-Richter statistics were generally pretty good, however a correct implementation of Gutenberg-Richter considers only the primary events. Therefore, we seek a catalog with aftershocks removed in order to improve our assessment of the Gutenberg-Richter statistics. The process to remove aftershocks is called declustering.\n",
    "\n",
    "In this exercise, you will evaluate a published declustering method as you use it to decluster the catalog analyzed above. Then you will re-compute the Gutenberg-Richter coefficients for the declustered catalog in order to examine the affect on the G-R statistics.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declustering Algorithm\n",
    "\n",
    "The analysis that was just performed was for the raw catalog, which means that it includes all events. However Gutenberg-Richter is really interested in the occurrence of primary \"main shock\" events, and therefore it is necessary to decluster the catalog to obtain an unbiased estimate of the G-N coefficients. Declustering here means remove the aftershocks from the catalog. This is done using an algorithm that relates the \"expected\" time and distance range of aftershocks from a given mainshock. Large mainshocks will result in aftershock populations that, statistically speaking, have a greater likelihood to occur over longer time periods and greater mainshock-aftershock distances compared with smaller mainshock-aftershock series. \n",
    "\n",
    "The code block below defines a declustering algorithm. This algorithm uses distance and time metrics that are magnitude dependent, called 'Dtest' and 'Ttest'. If a given event falls within the maximal values defined by Dtest and Ttest for its magnitude it is deemed an aftershock and removed from the catalog. After all events are processed, the remaining catalog is then comprised of only primary events. This declustered catalog can be used to estimate more accurate Gutenberg-Richter statistics. Furthermore, we can study the aftershock events that the algorithm removed for a given earthquake in the context of the Omori Law statistics (Exercise 4).\n",
    "\n",
    "Because aftershock identification is an empirical procedure, there are many different ways to define the Dtest and Ttest relationships. Stiphout et al., (2012, on page 10) summarizes three different definitions of the Dtest/Ttest relationships originally proposed by Uhrhammer (1986), Knopoff and Gardner (1972), and Gruenthal. \n",
    "\n",
    "Compare the event reduction rate (final number divided by the initial number of events) for the three different proposed distance and time windows. You can do this by adding a logical (if statement) tree to enable switching between different definitions of Dtest and Ttest in declustering_algorithm below. The first definition (Eqn 1 from Stiphout et al., 2012, p.10) has already been completed. Note that some of these algorithms will not run instantaneously and might take up to a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declustering_algorithm(cat,definition=1):\n",
    "    '''\n",
    "    Decluster a catalog\n",
    "    \n",
    "    note: This function may take a few minutes to complete\n",
    "    \n",
    "    calls readAnssCatalog()\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    cat must be an anss formatted pandas datafram\n",
    "    definition is the algorithm (1 - 3) from Stiphout, 2012, which determines Dtest and Ttest values\n",
    "        Definition = 1 : Gardner and Knopoff, 1974 [default]\n",
    "        Definition = 2 : Gruenthal\n",
    "        Definition = 3 : Uhrhammer, 1986\n",
    "    \n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # do not edit\n",
    "    cnt=0\n",
    "    save=np.zeros((1,10000000),dtype=int)\n",
    "\n",
    "    # grab catalog arrays\n",
    "    year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(cat)\n",
    "    ne=len(year)\n",
    "\n",
    "    # main for-loop over events\n",
    "    for i in range(0,ne,1):\n",
    "        \n",
    "        if definition == 1:\n",
    "            \n",
    "            # Definition #1 : Knopoff and Gardner, 1972\n",
    "            Dtest=np.power(10,0.1238*mag[i]+0.983)\n",
    "            if mag[i] >= 6.5:\n",
    "                Ttest=np.power(10,0.032*mag[i]+2.7389)\n",
    "            else:\n",
    "                Ttest=np.power(10,0.5409*mag[i]-0.547)\n",
    "\n",
    "        elif definition == 2:\n",
    "\n",
    "            # Definition #2 : Gruenthal ** seems to be an error in the paper **\n",
    "            Dtest=np.exp(1.77+(0.037+1.02*mag[i])**2)   # distance bounds\n",
    "            if mag[i] >= 6.5:\n",
    "                Ttest=abs(np.exp(-3.95+(0.62+17.32*mag[i])**2))  # aftershock time bounds for M >= 6.5\n",
    "            else:\n",
    "                Ttest=np.power(10,0.024*mag[i]+2.8)  # aftershock time bounds for M < 6.5\n",
    "    \n",
    "        elif definition == 3:\n",
    "\n",
    "            # Definition #3\n",
    "            Dtest= ...\n",
    "            Ttest= ...\n",
    "\n",
    "            \n",
    "            \n",
    "        a=days[i+1:ne]-days[i]\n",
    "        m=mag[i+1:ne]\n",
    "        b=haversine_np(lon[i],lat[i],lon[i+1:ne],lat[i+1:ne])\n",
    "\n",
    "        icnt=np.count_nonzero(a <= Ttest)\n",
    "        if icnt > 0:\n",
    "            itime=np.array(np.nonzero(a <= Ttest)) + (i+1)\n",
    "            for j in range(0,icnt,1):             \n",
    "                if b[j] <= Dtest and m[j] < mag[i]:\n",
    "                    save[0][cnt]=itime[0][j]\n",
    "                    cnt += 1 # save contains index of aftershocks in cat\n",
    "\n",
    "    #Note this is an array of indexes that will be used to delete events flagged \n",
    "                        #as aftershocks\n",
    "    save=np.delete(np.unique(save),0)  \n",
    "    \n",
    "    # Filter or slice out the declustered and aftershock dataframe catalogs from the \n",
    "    # original dataframe catalog \"data\" using \"save\" above.\n",
    "    cat_aftershocks = cat.iloc[...]\n",
    "    cat_declustered = cat.iloc[~cat.index.isin(save)]\n",
    "    \n",
    "    cat_aftershocks.reset_index(drop=True, inplace=True)\n",
    "    cat_declustered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return cat_declustered, cat_aftershocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the declustering algorithm\n",
    "method = 1\n",
    "data_declustered, data_aftershocks = declustering_algorithm(data,definition=method)\n",
    "\n",
    "# This condition should print out \"True\" if the catalogs were separated correctly\n",
    "len(data) == len(data_aftershocks) + len(data_declustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Declustering algorithm {method} removed {len(data_aftershocks)} events.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a map showing the declustered catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the declustered dataframe into numpy as an array with different d\"\" variable names  \n",
    "dyear,dmonth,dday,dhour,dmn,dsec,dlat,dlon,dmag,ddays = readAnssCatalog(data_declustered)\n",
    "\n",
    "#Make map\n",
    "lat0=36.75 #Set Corners of Map\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "ydim=10      #height of plot\n",
    "xdim=ydim*(haversine_np(lon0,lat0,lon1,lat0)/haversine_np(lon0,lat0,lon0,lat1)) #scale width\n",
    "plt.figure(figsize=(ydim,xdim))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='longitude', ylabel='Latitude',\n",
    "       title='Declustered Catalog')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES,alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "ax.add_feature(cfeature.STATES.with_scale('10m'))\n",
    "\n",
    "# Plot events as open circles with size and color proportional to event magnitude\n",
    "indx=np.argsort(dmag)   #determine sort index #Sort Descending to plot largest events on top\n",
    "x=dlon[indx]            #apply sort index\n",
    "y=dlat[indx]\n",
    "z=np.exp(dmag[indx])    #exponent to scale size\n",
    "c = plt.cm.plasma(z/max(z))\n",
    "plt.scatter(, , s=, facecolors='none', edgecolors=c, marker='o', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add Berkeley, CA as a red square with size proportional to event magnitude\n",
    "plt.plot(, ,'rs',markersize=10)\n",
    "\n",
    "#Save the plot by calling plt.savefig() BEFORE plt.show()\n",
    "# plt.savefig('hw1_ex4_seismap_declust.pdf')\n",
    "# plt.savefig('hw1_ex4_seismap_declust.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Re-compute the Gutenberg-Richter statistics as above for the declustered catalog\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine and plot the Gutenberg-Richter Distribution for De-clustered data\n",
    "#You may want to adjust the magnitude range of the analysis to focus on where the catalog is complete\n",
    "step = 0.01\n",
    "m = step * np.arange(1.5/step,(6.9+step)/step) # protecting against a numerical precision issue\n",
    "N = np.zeros(len(m))\n",
    "\n",
    "for i in range(0,len(m),1):\n",
    "    ...\n",
    "\n",
    "#Invert for A and B values\n",
    "G=np.column_stack(( ,m))\n",
    "GTG=np.dot(np.transpose(G),G)\n",
    "GTD=np.dot(np.transpose(G),N)\n",
    "soln=np.linalg.solve(, )\n",
    "x=m\n",
    "y=np.dot( ,soln)\n",
    "\n",
    "#Compute the uncertainty in Gutenberg-Richter Parameters\n",
    "df=len(N) - 2                 #degree of freedom\n",
    "e=N-y                         #prediction error\n",
    "var=np.sum(e**2)/df\n",
    "se_y=np.sqrt(var)             #standard error of the estimate\n",
    "sdev=np.sqrt(var)             #standard deviation\n",
    "\n",
    "#Calculate 95% confidence bounds\n",
    "t=stats.t.ppf(1-0.05/2,df)    #two-sided students t-distribution\n",
    "tmp=np.sqrt(1/len(x)+((x-np.mean(x))**2)/np.sum((x-np.mean(x))**2))\n",
    "tmp=tmp/max(tmp)\n",
    "lower95=y-t*se_y*tmp\n",
    "upper95=y+t*se_y*tmp\n",
    "se_b=sdev/np.sqrt(np.sum((x-np.mean(x))**2))                      #standard error slope\n",
    "se_a=sdev*np.sqrt(1/len(x) + np.mean(x)**2/np.sum((x-np.mean(x))**2)) #standard error of intercept\n",
    "a95=se_a*t\n",
    "b95=se_b*t\n",
    "\n",
    "#Now Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(m, N,'b.',x,y,'k-',x,lower95,'r-',x,upper95,'r-')\n",
    "ax.set(xlabel='magnitude', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Declustered Gutenberg-Richter Distribution')\n",
    "ax.grid()\n",
    "\n",
    "# fig.savefig(\"hw1_ex4_figure4.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f'A_value= {soln[0]:.3f} B_value={soln[1]:.3f}')\n",
    "print(f'95%intercept= {a95:.3f} 95%slope={b95:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. How many events were removed from the catalog by each declustering algorithm?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3-1\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Compare the spatial distribution of earthquakes between the raw and declustered catalogs.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3-2\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Compare the Gutenberg-Richter A and B coefficients for the declustered catalog using algorithms 1 and 3.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3-3\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. What is the annual rate of occurrence of M4 earthquakes for each of the declustered catalogs?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3-4\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. What is the average M7 return period (inverse of annual occurrence of M7 events) for each of the declustered catalogs?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3-5\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "6. Compare your estimated values with what has been presented in the USGS Earthquake Hazard Assessments of the return period for Hayward fault earthquakes.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3-6\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Omori Law for Loma Prieta M6.9 Event (30 pts)\n",
    "\n",
    "Here we will use the declustering algorithm to identify aftershocks of the October 18 1989 at 04:15am (October 17 at 5:15pm PDT) the M6.9 Loma Prieta earthquake occurred in the Santa Cruz mountains approximately 80 km southwest of the Berkeley Campus. This wiki has some background information for the earthquake: https://en.wikipedia.org/wiki/1989_Loma_Prieta_earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Earthquake Catalog\n",
    "\n",
    "Load the .csv data file of all the earthquakes 1900 - 2018 in the ANSS (Advanced National Seismic System) catalog from 100 km around Berkeley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This catalog is a M0+ search centered at Berkeley radius=100km. \n",
    "# A big enough radius to include Loma Prieta but exclude Geysers.\n",
    "data=pd.read_csv('anss_catalog_1900to2018all.txt', sep=' ', delimiter=None, header=None,\n",
    "                 names = ['Year','Month','Day','Hour','Min','Sec','Lat','Lon','Mag'])\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select earthquakes related to the Loma Prieta Earthquake\n",
    "\n",
    "Use Boolean indexing to select events from the full catalog from between October 18, 1989 (date of mainshock) and December 18, 1989 (3-months following)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_1989 = data[(data.Year>=) & (data.Year<)]        #get one year of data\n",
    "fall_eq = EQ_1989[(EQ_1989.Month>) & (EQ_1989.Month<=)]    #collect months of Oct, Nov and Dec\n",
    "LP_eq = fall_eq[(~((fall_eq.Month==10) & (fall_eq.Day<18)))]  #negate events before day (assumes first month is 10)\n",
    "LP_eq = LP_eq[(~((LP_eq.Month==12) & (LP_eq.Day>18)))]        #negate events after day (assumes last month is 12)\n",
    "LP_eq.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Create data arrays for 3-month period beginning with Loma Prieta Earthquake\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year,month,day,hour,minute,sec,lat,lon,mag = readAnssCatalog(EQ_1989)#override for plotting entire year catalog\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(LP_eq)\n",
    "nevt = len(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot the Loma Preita time series\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.2\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude vs. days from mainshock\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(  , ,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='Days', ylabel='Magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "ax.set_ylim([0,7])\n",
    "# fig.savefig(\"hw1_ex4_ts_raw.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Number={nevt:d} MinMag={min(mag):.2f} MaxMag={max(mag):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot the Loma Preita Earthquake Catalog in map view\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.3\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_aspect('auto')\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Earthquake Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(mag)   #determine sort index\n",
    "x=lon[indx]            #apply sort index\n",
    "y=lat[indx]\n",
    "z=np.exp(dmag[indx])    #exponent to scale size\n",
    "c = plt.cm.viridis_r(z/max(z)) # colormap scales with magnitude\n",
    "plt.scatter(x, y, s=(z/2), facecolors=c, alpha=0.5, edgecolors='k', marker='o', linewidth=2) # plot circles on EQs\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8)  # plot red square on Berkeley\n",
    "\n",
    "\n",
    "# plt.savefig(\"hw1_ex4_map_raw.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Decluster the Raw Catalog for the Loma Prieta time period\n",
    "\n",
    "We use the same decluster algorithm previously to identify aftershocks and remove them from the 30-day Loma Preita catalog.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.4\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec, data_after = declustering_algorithm(LP_eq,definition=3)\n",
    "\n",
    "# This condition should print out \"True\" if the catalogs were separated correctly\n",
    "len(LP_eq) == len(data_after) + len(data_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two new sets of event info arrays, one for the declustered catalog and one for the aftershock catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyear,dmonth,dday,dhour,dminute,dsec,dlat,dlon,dmag,ddays = readAnssCatalog(data_dec)\n",
    "ayear,amonth,aday,ahour,aminute,asec,alat,alon,amag,adays = readAnssCatalog(data_after)\n",
    "dnevt =len(ddays)\n",
    "anevt=len(adays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Aftershock Catalog in time series\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot( ,  ,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='days', ylabel='magnitude',\n",
    "       title='Aftershock Event Catalog')\n",
    "ax.grid()\n",
    "ax.set_ylim([0,7])\n",
    "# fig.savefig(\"hw1_ex4_ts_aftershockOnly.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Number={anevt:d} MinMag={min(amag):.2f} MaxMag={max(amag):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot the declustered Loma Prieta mainshock catalog in map view\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.5\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make a Map of the declustered events\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Mainshock Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(dmag)   #determine sort index\n",
    "x=dlon[indx]            #apply sort index\n",
    "y=dlat[indx]\n",
    "z=np.exp(dmag[indx])    #exponent to scale size\n",
    "c = plt.cm.viridis_r(z/max(z))\n",
    "plt.scatter(x, y, s=(z/2), facecolors=c, alpha=0.4, edgecolors='k', marker='o', linewidth=2)\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8)\n",
    "\n",
    "# plt.savefig(\"hw1_ex4_map_mainshockOnly.png\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot the declustered Loma Prieta aftershock catalog in map view\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.6\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of Aftershock events\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Aftershock Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(amag)   #determine sort index\n",
    "x=alon[indx]            #apply sort index\n",
    "y=alat[indx]\n",
    "z=np.exp(amag[indx])    #exponent to scale size\n",
    "c = plt.cm.plasma(z/max(z))\n",
    "plt.scatter(x, y, s=(z/2), facecolors=c, alpha=0.4, edgecolors='k', marker='o', linewidth=2)\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8)\n",
    "\n",
    "# plt.savefig(\"hw1_ex4_map_aftershockOnly.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Omori statistics\n",
    "\n",
    "To compute the Omori statistics we want to bin the log10 of the number of aftershocks each day following the mainshock and fit a power law equation such as:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "N=\\frac{A}{(t+\\epsilon)^P}, \n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "where t is time in days, N is the number of earthquakes in the 24 hour period, and $\\epsilon$ is a small number (fraction of a day) to avoid the singularity at zero time. A and P are the coeffients that we want to find through regression. This power law equation can be linearized by simply taking the log10 of both sides giving:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "log_{10}(N)=log_{10}A - P*log_{10}(t+\\epsilon)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note: We will use both the Gutenberg-Richter and the Omori Law statistics computed in Homework 1 in Homework 2 where we will examine the probability of earthquake occurrence and aftershock occurrence following a given mainshock.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.7\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of aftershocks in each day\n",
    "epsilon=0.1\n",
    "maxdays=np.int(np.max(adays))\n",
    "t=np.arange(1,maxdays+2,1)\n",
    "# t=np.arange(maxdays+1)\n",
    "logt=np.log10(t+epsilon)\n",
    "N=np.zeros(maxdays+1)\n",
    "\n",
    "for i in range(maxdays+1):\n",
    "    N[i]=np.count_nonzero(adays == i)\n",
    "logN=np.log10(N)\n",
    "\n",
    "#Invert for A and P values and result\n",
    "G= ...\n",
    "GTD= ...\n",
    "GTG=np.dot( , )\n",
    "GTD=np.dot( , )\n",
    "soln= ...\n",
    "x= ...\n",
    "y= ...\n",
    "\n",
    "print(f'A_value= {soln[0]:.3f} P_value={soln[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Plot linear\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t-epsilon, N,'bo',x,y,'r-',linewidth=2)\n",
    "ax.set(xlabel='days after mainshock', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Omori Law')\n",
    "ax.grid()\n",
    "# plt.savefig(\"hw1_ex4_OmoriStats_linlin.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Plot loglog\n",
    "fig, ax = plt.subplots()\n",
    "ax.loglog(t-epsilon, N,'bo',x,y,'r-',linewidth=2)\n",
    "ax.set(xlabel='days after mainshock', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Omori Law')\n",
    "ax.grid()\n",
    "# plt.savefig(\"hw1_ex4_OmoriStats_loglog.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Plot of the number of earthquakes per day for 1989\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create data arrays for the entirety of 1989\n",
    "year=EQ_1989.Year.values\n",
    "month=EQ_1989.Month.values\n",
    "day=EQ_1989.Day.values\n",
    "nevt=len(year)        #number of events \n",
    "\n",
    "#Determine the number of days from the first event\n",
    "days=np.zeros(nevt) # initialize the size of the array days\n",
    "\n",
    "for i in range(0,nevt,1):\n",
    "    d0 = datetime.date(year[0], month[0], day[0])\n",
    "    d1 = datetime.date(year[i], month[i], day[i])\n",
    "    delta = d1 - d0\n",
    "    days[i]=delta.days # fill days in with the number of days since the first event\n",
    "    \n",
    "maxdays=np.int(np.max(days))\n",
    "NN=np.zeros(maxdays+1)\n",
    "t=np.arange(0,maxdays+1,1)\n",
    "for i in range(0,maxdays+1,1):\n",
    "    NN[i]=np.count_nonzero(days == i)\n",
    "\n",
    "#Now Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(..., ...,'bo-',linewidth=2)\n",
    "ax.set(xlabel='day', ylabel='Number of Earthquakes',\n",
    "       title='Number of Earthquakes Per Day in 1989')\n",
    "ax.grid()\n",
    "# plt.savefig(\"hw1_ex4_omori.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. Which faults were active during Loma Preita?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-1\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. What could cause aftershocks to occur on faults other than the mainshock fault?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-2\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Here we used a 3-month period beginning at the Loma Prieta Earthquake. Examine the results taking a 6-month period beginning 3 months before the Loma Prieta earthquake. How does the distribution of earthquakes differ for the two time periods.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-3\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. How does the estimated P-value compare to values reported in Lay and Wallace?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-4\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. Is the aftershock more or less productive than average?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-5\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "6. What is the number of earthquakes per day in the region for the three-month period leading up to the Loma Prieta earthquake?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-6\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_eq = EQ_1989[(EQ_1989.Month>=...) & (EQ_1989.Month<...)]    #collect 3 months before LomaP\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(pre_eq)\n",
    "daily_rate= ...\n",
    "print(f'Daily rate prior to LP is approximately {daily_rate:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "7. How long after the earthquake does the applied Omori Law predict that the aftershock rate falls to the pre-event rate of earthquakes?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.8-7\n",
    "type: manual\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "T= ...\n",
    "print(f'Time to return to background={T:.1f} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a pdf file for you to submit. **Please save before exporting!** The exporter will not see any unsaved changes to your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../eps130_export eps130_hw1_gutenberg_richter_v2.0.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Access your pdf here.](./eps130_hw1_gutenberg_richter_v2.0.pdf)\n",
    "\n",
    "Remember to check that you pdf shows your most recent work before submitting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
