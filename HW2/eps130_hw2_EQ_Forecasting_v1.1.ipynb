{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"eps130_hw2_EQ_Forecasting_v1.1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Occurrence of Mainshocks and Aftershocks\n",
    "\n",
    "## Introduction\n",
    "In the previous homework we learned about the particularly well-behaved statistics of the earthquake magnitude distribution, and aftershock occurrence. As we saw, it is possible to use the frequency of event occurrence over a range of magnitudes to extrapolate to the less frequent large earthquakes of interest. How far this extrapolation may be extended depends upon a number of factors. It is certainly not unbounded as fault dimension, segmentation, strength and frictional properties will play a role in the maximum size earthquake that a fault will produce. Paleoseismic data is used to provide a better understanding of the recurrence of the large earthquakes of interest. The large earthquakes have greater fault offset, rupture to the surface of the Earth and leave a telltale geologic record. This record is used to determine the recurrence of the large characteristic earthquakes and probabilistic earthquake forecasts.  Finally, this type of analysis is perhaps one of the most visible products of earthquake hazard research in that earthquake forecasts and probabilities of aftershock occurrence are generally released to the public.\n",
    "\n",
    "## Objective\n",
    "In this homework we will assume a Poisson distribution to determine the probability of events based on the Gutenberg-Richter recurrence relationship.  Given the statistical aftershock rate model of Reasenberg and Jones (1996) we will forecast the probability of occurrence of large aftershocks for the 2014 Napa earthquake sequence. For the Mojave segment of the San Andreas Fault we will compare probability density models to the recurrence data and use the best fitting model to determine the 30-year conditionally probability of occurrence of a magnitude 8 earthquake. \n",
    "\n",
    "Use the code provided in this Jupyter Notebook to analyze the provided data, and then answer the questions to complete this homework. Submit your completed notebook in a \\*.pdf format. Write your answers embedded as Markdown inside the notebook where specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     17,
     26
    ]
   },
   "outputs": [],
   "source": [
    "#Initial Setup and Subroutine Definitions\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "def countDays(c,y,m,d):\n",
    "    days=np.zeros(c)\n",
    "    for i in range(0,c,1):\n",
    "        d0 = datetime.date(y[0], m[0], d[0])\n",
    "        d1 = datetime.date(y[i], m[i], d[i])\n",
    "        delta = d1 - d0\n",
    "        days[i]=delta.days\n",
    "    return days\n",
    "def readAnssCatalog(p):\n",
    "    # slices up an ANSS catalog loaded as a pandas dataframe and returns event info\n",
    "    d=np.array(p)         # load the dataframe into numpy as an array      \n",
    "    year=d[:,0].astype(int)  # define variables from the array\n",
    "    month=d[:,1].astype(int)\n",
    "    day=d[:,2].astype(int)\n",
    "    hour=d[:,3].astype(int)\n",
    "    minute=d[:,4].astype(int)\n",
    "    sec=d[:,5].astype(int)\n",
    "    lat=d[:,6]\n",
    "    lon=d[:,7]\n",
    "    mag=d[:,8]\n",
    "    days = countDays(len(year),year,month,day)\n",
    "    return year,month,day,hour,minute,sec,lat,lon,mag,days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The simplest model description of the probability that an earthquake of a given magnitude will happen is that of random occurrence. In fact when you examine the earthquake catalog it does in fact appear to be randomly distributed in time with the exception of aftershocks and a slight tendency of clustering. The Poisson distribution is often used to examine the probability of occurrence of an event within a given time window based on the catalog statistics. A Poisson process occurs randomly with no “memory” of time, size or location of any preceding event. Note that this assumption is inconsistent with the implications of elastic rebound theory applied to a single fault for large repeating earthquakes, but is consistent with the gross seismicity catalog.  \n",
    "\n",
    "The Poisson distribution is defined as,\n",
    "\n",
    "$$\n",
    "p(x)=\\frac{u^x e^{-u}}{x!}\n",
    "$$\n",
    "\n",
    "where $x$ is the number of events, and $u$ is the number of events that occur in time $\\delta t$ given the rate of event occurrence $\\lambda$, or $u = \\lambda*\\delta t$. Consider the case in which we would like to know the probability of an event of a certain magnitude occurring within a certain time. Using the Poisson distribution, we can define the probability of one or more events occuring to be,\n",
    "\n",
    "$$\n",
    "p(x >= 1)=1.0 - e^{-u}.\n",
    "$$\n",
    "\n",
    "The probability of one or more events occuring in a specified time period, for example $\\delta t =$ 30 years, can be shown to be\n",
    "\n",
    "$$\n",
    "p(x >= 1)=1.0 - e^{-\\lambda \\delta t},\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the annual rate of event occurrence (N), taken from Gutenberg-Richter analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1.1\n",
    "\n",
    "Using the Poisson model estimate the probability of a magnitude 5+ earthquake in a given week, month, year and 5 year period using the annual rate determined from the Gutenberg-Richter relationship for the Greater San Francisco Bay Area:\n",
    "\n",
    "$$\n",
    "Log(N) = 3.45 - 0.830M\n",
    "$$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this cell for questions 1 & 2, just skip ahead to the\n",
    "#    question 2 test cell when using the question 2 magnitude\n",
    "\n",
    "# Average annual rate of occurrence of M mag+ from G-R stats\n",
    "mag = ...\n",
    "lam = ...\n",
    "\n",
    "# Time range in years\n",
    "# You can compute all four (week, month, 1y, 5y) probabilities\n",
    "#    at once by making a list of time ranges in years\n",
    "dt = [ , , , ] # the tester expects these in ascending order [w,m,1y,5y]\n",
    "\n",
    "# The probability of an event of Magnitude M occurring\n",
    "P = [... for t in dt] # this is called a \"list comprehension\"\n",
    "# the tester expects plain probability values in the range [0,1]; don't convert to percentages\n",
    "\n",
    "print('The probability of an M5+ event occurring in 1 week, 1 month, 1 year, and 5 years=')\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1.2\n",
    "\n",
    "Compare the estimated probability of a magnitude 7.0+ earthquake for the same time periods.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The Poisson probability function above may also be used to determine the probability of one or more aftershocks of given magnitude range and time period following the mainshock. \n",
    "\n",
    "Typically an estimate of the probability of magnitude 5 and larger earthquakes is given for the period of 7 days following a large mainshock. This aftershock probability estimate is found to decay rapidly with increasing time.  Reasenberg and Jones (1989) studied the statistics of aftershocks throughout California and arrived at the following equation describing the rate of occurrence of one or more events as a function of elapsed time for a generic California earthquake sequence:\n",
    "\n",
    "$$\n",
    "rate(t,M)=10^{(-1.67 + 0.91*(Mm - M))}   * (t + 0.05)^{-1.08}, \n",
    "$$\n",
    "\n",
    "where Mm is the mainshock magnitude, M is magnitude of aftershocks (can be larger than Mm), and t is time in units of days. This equation describes the daily production rate of aftershocks with magnitude (M) after the mainshock with magnitude Mm. The rate is a function of time (t) and the aftershock magnitude. Elements of both the Gutenberg-Richter relationship and Omori’s Law are evident in the above equation.  \n",
    "\n",
    "The Poisson probability of one or more aftershocks with magnitude M in range of M1 < M < M2, and time t in range t1 < t < t2 is:\n",
    "\n",
    "$$\n",
    "p(M1,M2,t1,t2) = 1.0 - e^{-\\int_{M1}^{M2} \\int_{t1}^{t2}rate(t,M)dtdM}\n",
    "$$\n",
    "\n",
    "The double integral in the exponent may be approximated by nested summations. That is, for each magnitude from M1 to M2 the sum of the rate function over the time period of interest (typically from t1=0 to t2=7 days) can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the integral exactly for the number of earthquakes \n",
    "in the magnitude range [M1,M2] and time range [t1, t2]:\n",
    "\n",
    "$$\n",
    "p(M1,M2,t1,t2) = 1.0 - e^{-\\int_{M1}^{M2} \\int_{t1}^{t2}rate(t,M)dtdM}\n",
    "$$\n",
    "$$\n",
    "u = \\int_{M1}^{M2} \\int_{t1}^{t2}rate(t,M)dtdM\n",
    "$$\n",
    "$$\n",
    "u= \\int_{M1}^{M2} \\int_{t1}^{t2} 10^{(-1.67 + 0.91*(Mm - M))}   * (t + 0.05)^{-1.08} dtdM\n",
    "$$\n",
    "$$\n",
    "u= 10^{-1.67+0.91Mm}  \\int_{M1}^{M2} 10^{-0.91M}  dM \\int_{t1}^{t2} (t + 0.05)^{-1.08} dt\n",
    "$$\n",
    "$$\n",
    "u= \\frac{10^{-1.67+0.91Mm}}{ln(10)(-0.91) (-0.08)}  [10^{-0.91M_2} - 10^{-0.91M_1}][ (t_2 + 0.05)^{-0.08} - (t_1 + 0.05)^{-0.08}]\n",
    "$$\n",
    "\n",
    "Then the probability (p(x)) of having one of more earthquakes in the magnitude range [M1,M2] and time range [t1,t2] is:\n",
    "\n",
    "$$\n",
    "p = 1-e^{-u}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2.1\n",
    "Use these relationships to estimate the probability of one or more magnitude 5 and larger (potentially damaging) aftershocks in the 7 days following the October 18, 1989 Loma Prieta Earthquake studied in Homework 1.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Loma Prieta earthquake, Mm = 6.9, \n",
    "# M1 = 5.0 and M_2 = 6.8 (since the question asks for aftershocks, \n",
    "# the aftershock maximum magnitude should be less than the mainshock \n",
    "# magnitude, otherwise it will be mainshock and Loma Prieta earthquake will \n",
    "# be termed \"foreshock\".)\n",
    "#\n",
    "\n",
    "P = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2.2\n",
    "By the end of day two how much has the probability of occurrence of a magnitude 5+ aftershock decayed? That is, what is the new 7-day probability starting on day 2?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "We want to compare the expected number of aftershocks per day for various magnitude thresholds (M > 2, M > 3 etc) and the observed outcome for the Loma Prieta earthquake sequence. Start by making a table of the observed aftershocks per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog from HW #1 (provided in your current working directory)\n",
    "print('The magnitude-time distribution of Loma Prieta aftershocks is shown here:')\n",
    "data=pd.read_csv('anss_catalog_1900to2018all.txt', sep=' ', delimiter=None, header=None,\n",
    "                names = ['Year','Month','Day','Hour','Min','Sec','Lat','Lon','Mag'])\n",
    "EQ_1989 = data[(data.Year>=1989) & (data.Year<1990)]          #get one year of data\n",
    "fall_eq = EQ_1989[(EQ_1989.Month>9) & (EQ_1989.Month<=12)]    #collect months of Oct, Nov and Dec\n",
    "LP_eq = fall_eq[(~((fall_eq.Month==10) & (fall_eq.Day<18)))]  #negate events before day (assumes first month is 10)\n",
    "LP_eq = LP_eq[(~((LP_eq.Month==12) & (LP_eq.Day>18)))]        #negate events after day (assumes last month is 12)\n",
    "LP_eq.reset_index(drop=True, inplace=True)\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(LP_eq)\n",
    "days = days[1:] # remove mainshock\n",
    "mag = mag[1:]\n",
    "\n",
    "# Plot of magnitude vs. day for entire catalog\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "ax.plot(days,mag,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='Days', ylabel='Magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "ax.set_ylim([0,7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count aftershocks each day from 10/18 to 10/25 and make a table aftershocks_observed\n",
    "aftershock_days = np.arange(18,26) # day dates\n",
    "aftershock_mags = np.arange(2,6) # mags to count\n",
    "aftershocks_observed = pd.DataFrame(columns = [f'10/{d}' for d in aftershock_days],\n",
    "                                    index=[f'M>={m}' for m in aftershock_mags]) # set up table\n",
    "\n",
    "# Fill in the table with the number of aftershocks per day\n",
    "# Hint: the easiest way to find the number of aftershocks per day in a magnitude range is to \n",
    "# further refine the LP_eq catalog uisng boolean statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aftershocks_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "We want to compare the expected number of aftershocks per day for various magnitude thresholds (M > 2, M > 3 etc) and the observed outcome for the Loma Prieta earthquake sequence. Now compute the expected number of aftershocks per day from the analytical integral of the rate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mm = 6.9\n",
    "\n",
    "\n",
    "\n",
    "aftershocks_RJ = pd.DataFrame(columns = [f'10/{d}' for d in aftershock_days],\n",
    "                              index=[f'M>={m}' for m in aftershock_mags]) # set up rate table\n",
    "\n",
    "\n",
    "def RJ(Mm,M1,M2,t1,t2):\n",
    "    u = ...\n",
    "    return int(np.round(u,0))\n",
    "\n",
    "# fill in aftershocks_RJ table\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "aftershocks_RJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aftershocks_observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2.5\n",
    "The statistics compiled by Reasenberg and Jones also allows for the estimation of the probability of an event larger than the mainshock occurring, or in other words the probability that a given event is in fact a foreshock. Immediately following the Loma Prieta earthquake, after a lapse time of 0.1 day, what was the 7-day probability that a larger earthquake might occur?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ...\n",
    "print('After 0.1 days, the probability that the Loma Prieta M6.9 earthquake was a foreshock to a larger earthquake was')\n",
    "print(str(round(P,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.6\n",
    "Practically speaking, what was the duration of the Loma Prieta sequence? Explain you answer in terms Omori statistics and the probability of aftershock occurrence with increasing time following the main shock. This is an open-ended question. You might compare pre-event and omori-decay seismicity rates. You could use Reasenberg and Jones to find a time when the probability of a felt earthquake has fallen to a low level.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.6\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can answer this by 1) comparing pre-event and omori decay rates, 2) and from reasonberg and jones finding the\n",
    "#time when the probability of say a M3+ earthquake falls to a low level. i.e. integrate from say t1 to t2=infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "As discussed in class paleoseismic trench data at Pallet Creek in southern California reveals the quasi-periodic behavior of large earthquakes on the San Andreas fault. From the very careful mapping of offset stratigraphy in the trench and carbon-14 radiometric dating these large earthquakes have been found to have occurred in 1857, 1812, 1480, 1346, 1100, 1048, 997, 797, 734, 671, 529 (see figure from Sieh, K., Stuiver, M. and Brillinger, D., 1989). These earthquakes include M8 earthquakes on the southern segment of the San Andreas fault, which extends from Parkfield southward through the Big Bend into southern California. Each earthquake may not have been as large as M8, however, given the mapped slip, each event is considered to be M>7. The 1857 earthquake was M8. \n",
    "\n",
    "<img src=\"palletCreek.png\">\n",
    "\n",
    "Using this recurrence data we are going to examine the periodicity, plot the distribution of events in terms of binned interval time, compare the observed distribution with idealized probability density functions, and then use those functions to estimate the conditional probability of occurrence of these events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.1\n",
    "\n",
    "Given the time intervals separating the event list given above, compare the fits of a Gaussian and Lognormal probability density model. \n",
    "\n",
    "Gaussian:  \n",
    "$$\n",
    "pd(u)=\\frac{e^{\\frac{-(u - T_{ave})^2}{2 {\\sigma}^2}}}{\\sigma \\sqrt{2 \\pi}}\n",
    "$$\n",
    "\n",
    "\n",
    "Log-Normal: \n",
    "$$\n",
    "pd(u)=\\frac{e^{\\frac{-{(ln(u/T_{ave}))}^2}{2 {(\\sigma / T_{ave})}^2}}}{(\\frac{\\sigma}{T_{ave}}) u  \\sqrt{2 \\pi}}\n",
    "$$\n",
    "\n",
    "The models depend on the mean interval recurrence time ($T_{ave}$), the standard deviation to the mean ($\\sigma$), and the random variable ($u$) which in this case represents the interval time.\n",
    "\n",
    "\n",
    "To do this make a histogram with bins from 1-51, 51-101, 101-151, etc. The center dates of the bins will be 26, 76, 126, etc. Then fit each probability density model. This part is done for you.\n",
    "\n",
    "**Question**: Which type of distribution appears to fit the data better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hint: matplotlib.pyplot and pandas.DataFrame both have \n",
    "#       histogram functions\n",
    "\n",
    "# Enter the event years and intervals into a table\n",
    "# There are other (better) ways to do this, can you think of one?\n",
    "print('\\nInterval Times:')\n",
    "c = {0:[1857,45],1:[1812,332],2:[1480,134],3:[1346,246],4:[1100,52],5:[1048,51],6:[997,200],7:[797,63],\n",
    "     8:[734,63],9:[671,142],10:[529,0]}\n",
    "df = pd.DataFrame.from_dict(data=c,orient='index',columns=['Date','Intervals'])\n",
    "print(df)\n",
    "\n",
    "# With so few data points we can get away with manually counting the bins\n",
    "# Think about how you could make python do more of the work here\n",
    "print('\\nHistogram Data:')\n",
    "c = {0:['0<=T<51',1],1:['51<=T<101',4],2:['101<=T<151',2],3:['151<T<201',1],\n",
    "     4:['201<T<251',1],5:['251<T<301',0],6:['301<T<351',1]}\n",
    "hf = pd.DataFrame.from_dict(data=c,orient='index',columns=['Time Range','Count'])\n",
    "print(hf)\n",
    "\n",
    "# Models\n",
    "Tave = np.mean(df.Intervals) # mean of each bin\n",
    "sig =  np.std(df.Intervals)\n",
    "u=np.arange(0.1,351,1,) # number of years spanned by all bins\n",
    "# Gaussian\n",
    "uG=np.exp(-(u - Tave)**2/(2*sig**2))/(sig*np.sqrt(2*np.pi))\n",
    "# Log-normal\n",
    "uLN=np.exp(-(np.log(u /Tave))**2/(2*(sig/Tave)**2))/((sig/Tave)*u*np.sqrt(2*np.pi))\n",
    "\n",
    "# Plot the result\n",
    "plt.figure()\n",
    "hf.Count.plot(kind='bar');\n",
    "plt.plot(u*(6/351),uG*500,'r-'); # scaling u and uG to match bar plot dimensions\n",
    "plt.plot(u*(6/351),uLN*500,'b-');\n",
    "plt.xticks(range(len(hf)), hf['Time Range'].values, size='small',rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 4\n",
    "In this problem we will estimate the probability of occurrence of a magnitude M8 earthquake based on the historic Pallet Creek recurrence data and the best fitting probability density model determined in Exercise 3.  \n",
    "\n",
    "The probability that an event will occur within a given time window, for example 30-years, is the definite integral of the probability density function computed over that time window: \n",
    "$$\n",
    "P(T_e <= T <= T_e + \\Delta T)=\\int_{T_e}^{T_e + \\Delta T} pd(u)du,\n",
    "$$\n",
    "\n",
    "where $\\Delta T$ is the length of the forecast window and $T_e$ is the time since the previous event. Note how P varies as a function of elapsed time. For any given forecast window, the value of P is small but is greatest near the mean of the distribution. Note that the Gaussian and lognormal probability density functions defined above are normalized to unit area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Estimate the 10-year, 20-year and 30-year probabilities for a repeat of this large Pallet Creek fault segment event using your estimates of $T_{ave}$, $\\sigma$, and $T_e=164$ years (time since 1857).\n",
    "\n",
    "The first step is to find the probability that the event will occur in the window, $\\Delta T$, with the condition that the event did not occur before $T_e$. This effectively reduces the sample space. The result is the following normalization for the conditional probability:\n",
    "\n",
    "$$\n",
    "P(T_e <= T <= T_e + \\Delta T | T >= T_e) = \\frac{\\int_{T_e}^{T_e + \\Delta T} pd(u)du}{1.0 - \\int_{0}^{T_e}pd(u)du}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = ...\n",
    "Tave = ...\n",
    "sig = ...\n",
    "\n",
    "# suggestion: make functions to calculate uG and uLN that you can use again in later questions\n",
    "def calc_uG(Tave,sig,u):\n",
    "    uG= ...\n",
    "    return uG\n",
    "def calc_uLN(Tave,sig,u):\n",
    "    uLN= ...\n",
    "    return uLN\n",
    "\n",
    "u= ...\n",
    "uG = calc_uG(Tave,sig,u)\n",
    "uLN = ...\n",
    "\n",
    "# if we use a step size of 1 (year) then we can numerically integrate by just taking the sum\n",
    "pG10_Te = np.sum(uG[Te:(Te+10)])/(1-np.sum(uG[0:Te]))\n",
    "pLN10_Te = ...\n",
    "pG20_Te = ...\n",
    "pLN20_Te = ...\n",
    "pG30_Te = ...\n",
    "pLN30_Te = ...\n",
    "print(\"Gaussian model\")\n",
    "print(\"{:.6f}, {:.6f}, {:.6f}\".format(pG10_Te,pG20_Te,pG30_Te))\n",
    "print(\"Log-normal model\")\n",
    "print(\"{:.6f}, {:.6f}, {:.6f}\".format(pLN10_Te,pLN20_Te,pLN30_Te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "These are the conditional probabilities of an earthquake occurring within a time interval of $\\Delta T$ years between $T_e$ And $T_e$+$\\Delta T$ years given that it did not occur before time $T_e$ (for $\\Delta T$ = 10 years, 20 years, and 30 years).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.2\n",
    "\n",
    "Make two plots showing (a) both pd(u) models for u = [0,500] years and (b) the 10-year, 20-year and 30-year probability windows for $T_e = [0,500]$ years (done for you). Describe the second plot. What does it tell you?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.2\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Models over 500 years\n",
    "plt.figure()\n",
    "plt.plot(u,uG,label='Gaussian')\n",
    "plt.plot(u,uLN,label='Log-Normal')\n",
    "plt.xlim([0,500])\n",
    "plt.ylim([0,max(uLN)])\n",
    "plt.legend()\n",
    "plt.xlabel('Interval time [years]')\n",
    "plt.ylabel('Number of Events')\n",
    "\n",
    "# We can integrate the definite integrals described above, using \n",
    "# Gaussian and Log-Normal distributions for pd(u), by np.trapz, np.sum, etc.\n",
    "te = range(0,5000,1)\n",
    "pG10 = np.zeros(np.shape(te))\n",
    "pLN10 = np.zeros(np.shape(te))\n",
    "for t in te:\n",
    "    uG = calc_uG(Tave,sig,u)\n",
    "#     print(np.shape(uG))\n",
    "    pG10[t] = np.sum(uG[t:t+10])\n",
    "    uLN = calc_uLN(Tave,sig,u)\n",
    "    pLN10[t] = np.sum(uLN[t:t+10])\n",
    "\n",
    "pG20 = np.zeros(np.shape(te))\n",
    "pLN20 = np.zeros(np.shape(te))\n",
    "for t in te:\n",
    "    uG = calc_uG(Tave,sig,u)\n",
    "    pG20[t] = np.sum(uG[t:t+20])\n",
    "    uLN = calc_uLN(Tave,sig,u)\n",
    "    pLN20[t] = np.sum(uLN[t:t+20])\n",
    "    \n",
    "pG30 = np.zeros(np.shape(te))\n",
    "pLN30 = np.zeros(np.shape(te))\n",
    "for t in te:\n",
    "    uG = calc_uG(Tave,sig,u)\n",
    "    pG30[t] = np.sum(uG[t:t+30])\n",
    "    uLN = calc_uLN(Tave,sig,u)\n",
    "    pLN30[t] = np.sum(uLN[t:t+30])\n",
    "    \n",
    "# Plot Probabilities\n",
    "plt.figure()\n",
    "plt.plot(u,pG10,'-',color='r',label='10-year Gaussian');\n",
    "plt.plot(u,pLN10,'-',color='b',label='10-year Log-Normal');\n",
    "plt.plot(u,pG20,'--',color='r',label='20-year Gaussian');\n",
    "plt.plot(u,pLN20,'--',color='b',label='20-year Log-Normal');\n",
    "plt.plot(u,pG30,':',color='r',label='30-year Gaussian');\n",
    "plt.plot(u,pLN30,':',color='b',label='30-year Log-Normal');\n",
    "plt.vlines(x=Tave,ymin=0,ymax=(max(pLN30)),linestyles='-',label='$T_{ave}$')\n",
    "\n",
    "plt.xlim([0,500])\n",
    "plt.ylim([0,max(pLN30)])\n",
    "plt.xlabel('Te [years]');\n",
    "plt.ylabel('Probability');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 4.3\n",
    "\n",
    "Estimate the change in the 30-year probability if the event does not occur in the next 10 years.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = ...\n",
    "pLN30_Te = ...\n",
    "print(f'{pLN30_Te:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.4\n",
    "\n",
    "Can you identify a weakness of this model?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4.4\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a pdf file for you to submit. **Please save before exporting!** The exporter will not see any unsaved changes to your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../eps130_export eps130_hw2_EQ_Forecasting_v1.0.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Access your pdf here.](./eps130_hw2_EQ_Forecasting_v1.0.pdf)\n",
    "\n",
    "Remember to check that you pdf shows your most recent work before submitting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
